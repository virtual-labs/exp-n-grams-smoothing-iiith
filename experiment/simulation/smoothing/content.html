<!-- This file needs to be edited by the lab developer to suit
the requirements of their lab in particular.--><!-- Add class="default" to include any element as it is
specified in default.html. 
Do not include class="default" to the elements that you want to
edit --><!DOCTYPE html>
<html><head><!--Google Tag Manager--><script class="gtm">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W59SWTR');</script><!--End Google Tag Manager--></head>
<body><!--Google Tag Manager (noscript)--><noscript class="gtm"><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-W59SWTR" style="display:none;visibility:hidden" width="0"></iframe></noscript><!--End Google Tag Manager (noscript)-->

<div id="experiment"> <!-- The Experiment Document Container-->

  <!-- The lab Header contains the logo and the name of the lab,
  usually displayed on the top of the page-->

  <header class="default" id="experiment-header">
  
    <div class="logo" id="experiment-header-logo">
      <!-- Enclose the logo image of your lab or write it in 
      text-->
      <img src="../images/logo.jpg"/>
    </div>

    <div class="heading" id="experiment-header-heading">
      <!-- Write the name of your lab and link it to the home 
      page of your lab (h1 tag is preferred while writing your 
      lab name)-->
      <a href="../index.html">Natural Language Processing Lab</a>	
    </div>

    <!-- Add any additional element you want to add to the lab 
    header, For example : Help (Enclosing them with suitable 
    div is recommended)-->

  </header>


  <!-- The lab article is the main content area where all the 
  experiment content sits-->
  <article id="experiment-article">
  
    <!-- The lab article has an header, optional navigational 
    menu, number of sections, an optional sidebar and a closing 
    footer-->
     <div class="breadcrumb" id="experiment-article-breadcrumb">
     </div>
    
      <header class="heading" id="experiment-article-heading">
        <!-- You can add a welcome message or title of the 
        experiment here -->
        N-Grams Smoothing
        <!-- Add any additional element if required with proper 
        enclosing-->
      </header>

      <!-- Navigation menu is useful to organize the view of 
      multiple sections inside the article-->
      <nav class="default" id="experiment-article-navigation">
        <ul id="experiment-article-navigation-menu">
          <!-- The menu can be dynamically generated to contain 
          the headings of your sections or instead write the 
          menu items of your choice individually enclosedu in 
          <li> tag as shown below-->
        </ul>
      </nav>

      <!-- All the sections of your lab or experiment can be 
      enclosed together with a div element as shown below-->
      <div id="experiment-article-sections">

        <!-- First section of the article-->
        <section id="experiment-article-section-1">
          
          <div class="icon" id="experiment-article-section-1-icon">
	    <!-- Enclose the icon image of your lab -->
	    <img src="../images/introduction.jpg"/>
	  </div>	
          
          <!-- The heading for the section can be enclosed in a 
          div tag. -->
          <div class="heading" id="experiment-article-section-1-heading">
            Introduction
          </div>

          <!-- Write the section content inside a paragraph 
          element, You can also include images with <img> tag -->
          <div class="content" id="experiment-article-section-1-content">	
One major problem with standard N-gram models is that they must be trained from some corpus, and because any particular training corpus is finite, some perfectly acceptable N-grams are bound to be missing from it. We can see that bigram matrix for any given training corpus is sparse. There are large number of cases with zero probabilty bigrams and that should really have some non-zero probability. This method tend to underestimate the probability of strings that happen not to have occurred nearby in their training corpus.
<br/><br/>
There are some techniques that can be used for assigning a non-zero probabilty to these 'zero probability bigrams'. This task of reevaluating some of the zero-probability and low-probabilty N-grams, and assigning them non-zero values, is called smoothing.

<br/><br/>
<center><img alt="1_alt" src="Exp10/a.jpg" style="height:250px; width:600px"/></center><br/>
<br/><br/><hr/>
        </div>


      </section>

      <!-- Second section of the article-->
      <section id="experiment-article-section-2">
        
        <div class="icon" id="experiment-article-section-2-icon">
	  <!-- Enclose the icon image of your lab. -->
	  <img src="../images/theory.jpg"/>
	</div>
				
        <!-- The heading for the section can be enclosed in a 
        div tag. -->
        <div class="heading" id="experiment-article-section-2-heading">
          Theory
        </div>


        <!-- Write the section content inside a paragraph 
        element, we can also include images with <img> tag -->
<div class="content" id="experiment-article-section-2-content">
<p>
The standard N-gram models are trained from some corpus. The finiteness of the training corpus leads to the absence of some perfectly acceptable N-grams. This results in sparse bigram matrices. This method tend to underestimate the probability of strings that do not occur in their training corpus.
<br/> <br/>
There are some techniques that can be used for assigning a non-zero probabilty to these 'zero probability bigrams'. This task of reevaluating some of the zero-probability and low-probabilty N-grams, and assigning them non-zero values, is called smoothing. Some of the techniques are: Add-One Smoothing, Witten-Bell Discounting, Good-Turing Discounting.<br/><br/>

</p><h4>Add-One Smoothing </h4><br/>

In Add-One smooting, we add one to all the bigram counts before normalizing them into probabilities. This is called add-one smoothing.<br/><br/>

<h4>Application on unigrams </h4><br/>
The unsmoothed maximum likelihood estimate of the unigram probability can be computed by dividing the count of the word by the total number of word tokens N

<pre>P(w<sub>x</sub>) = c(w<sub>x</sub>)/sum<sub>i</sub>{c(w<sub>i</sub>)}
      = c(w<sub>x</sub>)/N
</pre>

<br/>
Let there be an adjusted count c<sup>*</sup>.<br/>
c<sub>i</sub><sup>*</sup> = (c<sub<i< sub="">+1)*N/(N+V)<br/>
where where V is the total number of word types in the language.
<br/>
Now, probabilities can be calculated by normalizing counts by N.<br/>
p<sub>i</sub><sup>*</sup> = (c<sub<i< sub="">+1)/(N+V) <br/><br/>

<h4>Application on bigrams </h4><br/>
Normal bigram probabilities are computed by normalizing each row of counts by the unigram count:<br/>
P(w<sub>n</sub>|w<sub>n-1</sub>) = C(w<sub>n-1</sub>w<sub>n</sub>)/C(w<sub>n-1</sub>)<br/><br/>
For add-one smoothed bigram counts we need to augment the unigram count by the number of total word types in the vocabulary V:<br/>
p<sup>*</sup>(w<sub>n</sub>|w<sub>n-1</sub>) = ( C(w<sub>n-1</sub>w<sub>n</sub>)+1 )/( C(w<sub>n-1</sub>)+V )<br/><br/><hr/>
</sub<i<></sub<i<></div>
      </section>


      <section id="experiment-article-section-3">
        
        <div class="icon" id="experiment-article-section-3-icon">
	  <!-- Enclose the icon image of your lab. -->
	  <img src="../images/objective.jpg"/>
	</div>
     
        <div class="heading" id="experiment-article-section-3-heading">
          Objective
        </div>

        <div class="content" id="experiment-article-section-3-content">
<hr/><br/>
The objective of this experiment is to learn how to apply add-one smoothing on sparse bigram table.
<br/><br/><hr/>
        </div>

      </section>


      <section id="experiment-article-section-4">

        <div class="icon" id="experiment-article-section-4-icon">
	  <!-- Enclose the icon image of your lab.-->
	  <img src="../images/simulation.jpg"/>
	</div>

        <div class="heading" id="experiment-article-section-4-heading">
          Experiment
        </div>

        <div class="content" id="experiment-article-section-4-content">
<div id="smoothing"></div>
        </div>

      </section>


        <section id="experiment-article-section-5">
      
          <div class="icon" id="experiment-article-section-5-icon">
	    <!-- Enclose the icon image of your lab.-->
	    <img src="../images/quizzes.jpg"/>
	  </div>

          <div class="heading" id="experiment-article-section-5-heading">
            Quizzes
          </div>

          <div class="content" id="experiment-article-section-5-content">
<p>
Q1. Add-one smoothing works horribly in practice because of giving too much probability mass to unseen n-grams. Prove using an example. <br/>
Q2. In Add-δ smoothing, we add a small value 'δ' to the counts instead of one. Apply Add-δ smoothing to the below bigram count table where δ=0.02.<br/><br/>

</p><table border="1" cellpadding="4" cellspacing="-2" id="quiz-smoothing" style="text-align:center">
<tbody><tr><td></td>
<td><b>(eos)</b></td><td><b> John</b></td><td><b> read</b></td><td><b> Fountainhead</b></td><td><b> Mary</b></td><td><b> a</b></td><td><b> different</b></td><td><b> book</b></td><td><b> She</b></td><td><b> by</b></td><td><b> Dickens</b></td></tr><tr><td><b>(eos)</b></td><td>0</td><td>300</td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td></tr><tr><td><b> John</b></td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> read</b></td><td>0</td><td>0</td><td>0</td><td>300</td><td>0</td><td>600</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> Fountainhead</b></td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> Mary</b></td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> a</b></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>300</td><td>300</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> different</b></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> book</b></td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>300</td><td>0</td></tr><tr><td><b> She</b></td><td>0</td><td>0</td><td>0</td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><b> by</b></td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>300</td></tr><tr><td><b> Dickens</b></td><td>300</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table><br/><p style="font-size:130%">N = 5100   V = 11</p><br/><br/>
Q3. Given S = Dickens read a book, find P(S) <br/>
	(a) using unsmoothed probability <br/>
	(b) applying Add-One smoothing. <br/>
	(c) applying Add-δ smoothing <br/>
<p></p>
        </div></section>

        <section id="experiment-article-section-6">
	  
          <div class="icon" id="experiment-article-section-6-icon">
	    <!-- Enclose the icon image of your lab. -->
	    <img src="../images/procedure.jpg"/>
	  </div>
	
          <div class="heading" id="experiment-article-section-6-heading">
	    Procedure
	  </div>
	
          <div class="content" id="experiment-article-section-6-content">
<b><u>STEP1: </u></b>Select a corpus<br/>
<b><u>STEP2: </u></b>Apply add one smoothing and calculate bigram probabilities using the given bigram counts,N and V. Fill the table and hit <button>Submit</button><br/>
<b><u>STEP3: </u></b>If incorrect (red), see the correct answer by clicking on show answer or repeat Step 2 <br/> <br/><hr/>
	  </div>
	
        </section>
			
		
        <section id="experiment-article-section-7">
   
          <div class="icon" id="experiment-article-section-7-icon">
	    <!-- Enclose the icon image of your lab.-->
	    <img src="../images/readings.jpg"/>
	  </div>

          <div class="heading" id="experiment-article-section-7-heading">
            Further Readings
          </div>

          <div class="content" id="experiment-article-section-7-content">
<p></p><center><b>Speech and Language Processing - <i>An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition</i></b><br/>
BY: Daniel Jurafsky and James H. Martin<br/>
<i>Chapter 6</i></center><p></p>
<br/>
<br/>
          </div>

        </section>

      </div>


    <!-- An article can have a sidebar that contain related 
    links and additional material (however it is kept optional 
    at this moment) -->
    <aside class="default" id="lab-article-sidebar">
      <!-- put the content that you want to appear in the 
      sidebar -->	
    </aside>


    <!-- Article footer can display related content and 
    additional links -->						
    <footer class="default" id="footer"><div class="footer-top" id="contact">
  <div class="container">
    <div class="row">
      <div class="col-lg-4 col-md-6">
        <h4>Community Links</h4>
        <p>
	  <a href="http://www.sakshat.ac.in/">Sakshat Portal</a>
	</p>
        <p>
	  <a href="http://outreach.vlabs.ac.in/">Outreach Portal</a>
	</p>
        <p>
	  <a href="http://vlab.co.in/faq">FAQ : Virtual Labs</a>
	</p>
      </div>
      <div class="col-lg-4 col-md-6">
        <h4>Contact Us</h4>
	<p> <strong>Phone:</strong> General Information : 011-26582050 </p>
	<p> <strong>Email:</strong> support@vlab.co.in </p>
      </div>
      <div class="col-lg-4 col-md-6">
	<h4>Follow Us</h4>
	<div class="social-links">
	  <a class="twitter" href="https://twitter.com/TheVirtualLabs" style="background: #55acee;">
	    <i class="fa fa-twitter"></i>
	  </a>
	  <a class="facebook" href="https://www.facebook.com/Virtual-Labs-IIT-Delhi-301510159983871/" style="background: #3b5998;">
	    <i class="fa fa-facebook"></i>
	  </a>
	  <a class="google-plus" href="https://www.youtube.com/watch?v=asxRaOgk6a0" style="background: #e52d27;">
	    <i class="fa fa-youtube"></i>
	  </a>
	  <a class="linkedin" href="https://in.linkedin.com/in/virtual-labs-008ba9136" style="background: #2867B2;">
	    <i class="fa fa-linkedin"></i>
	  </a>
	</div>
      </div>
    </div>
  </div>
</div>
</footer>

  </article>


  <!-- Links to other labs, about us page can be kept the lab 
  footer-->
  <footer class="default" id="lab-footer">
    <!-- Put the content here-->
  </footer>

</div>		



</body></html>